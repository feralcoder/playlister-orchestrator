heat_template_version: 2018-08-31

description: >
  Template to deploy a layer of front-middle-ends with
  a cassandra backend

parameters:
  fe_name:
    type: string
    label: Front-Middle-End Instance Name
    description: Name to be used for all python service instance
    default: playlister_fe
  fe_sg_port_mins_tcp:
    type: comma_delimited_list
    label: Front-Middle-End Ports, TCP, Mins
    description: Port range mins for python services to receive, TCP
    default: "80,443,8000,8080,8443,22"
  fe_sg_port_maxes_tcp:
    type: comma_delimited_list
    label: Front-Middle-End Ports, TCP, Maxes
    description: Port range maxes for python services to receive, TCP
    default: "80,443,8000,8080,8443,22"
  fe_sg_port_mins_udp:
    type: comma_delimited_list
    label: Front-Middle-End Ports, UDP, Mins
    description: Port range udp for python services to receive, UDP
    default: ""
  fe_sg_port_maxes_udp:
    type: comma_delimited_list
    label: Front-Middle-End Ports, UDP, Maxes
    description: Port range maxes for python services to receive, UDP
    default: ""
  oltp_name:
    type: string
    label: Cassandra Instance Name
    description: Name to be used for all cassandra instance
    default: playlister_be
  olap_name:
    type: string
    label: Cassandra Instance Name
    description: Name to be used for all cassandra instance
    default: playlister_be

  # UGH - mins/maxes is a HACK!
  # 3306 - Standard SQL Port
  # 4444 - Galera State snapshot transfer
  # 4567 - Galera Cluster Communication
  # 4568 - Galera Incremental State Transfer
  # 8600-8630 - ColumnStore
  # 8700 - ColumnStore
  # 8800 - ColumnStore
  olap_sg_port_mins_tcp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, TCP, Mins
    description: Port range starts for mariadb columnstore service to receive, TCP
    default: "22,3306,8600,8700,8800"
  olap_sg_port_maxes_tcp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, TCP, Maxes
    description: Port range ends for mariadb columnstore service to receive, TCP
    default: "22,3306,8630,8700,8800"
  olap_sg_port_mins_udp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, UDP, Mins
    description: Port range starts for mariadb columnstore service to receive, UDP
    default: ""
  olap_sg_port_maxes_udp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, UDP, Maxes
    description: Port range ends for mariadb columnstore service to receive, UDP
    default: ""
  oltp_sg_port_mins_tcp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, TCP, Mins
    description: Port range starts for mariadb columnstore service to receive, TCP
    default: "22,3306,4444,4567,4568"
  oltp_sg_port_maxes_tcp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, TCP, Maxes
    description: Port range ends for mariadb columnstore service to receive, TCP
    default: "22,3306,4444,4567,4568"
  oltp_sg_port_mins_udp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, UDP, Mins
    description: Port range starts for mariadb columnstore service to receive, UDP
    default: "4567"
  oltp_sg_port_maxes_udp:
    type: comma_delimited_list
    label: MariaDB ColumnStore Ports, UDP, Maxes
    description: Port range ends for mariadb columnstore service to receive, UDP
    default: "4567"
  private_network:
    type: string
    description: Network used by the servers
    default: admin1-net
    constraints:
    - custom_constraint: neutron.network
  public_network:
    type: string
    description: Network used by the load balancer
    default: public1
    constraints:
    - custom_constraint: neutron.network
  vip_subnet:
    type: string
    description: Subnet on which the load balancer will be located
    default: public1-subnet
    constraints:
    - custom_constraint: neutron.subnet
  fe_vip:
    type: string
    description: IP of frontend VIP
    constraints:
    default: "172.30.1.11"
  olap_vip:
    type: string
    description: IP of frontend VIP
    constraints:
    default: "172.30.1.21"
  oltp_vip:
    type: string
    description: IP of frontend VIP
    constraints:
    default: "172.30.1.31"


resources:
  frontends:
    type: OS::Heat::AutoScalingGroup
    properties:
      resource:
        type: frontend.yaml
        properties:
          fe_secgroup_tcp_id: { get_resource: fe_secgroup_tcp }
          fe_secgroup_udp_id: { get_resource: fe_secgroup_udp }
          fe_80_pool_id: { get_resource: fe_80_pool }
          fe_8000_pool_id: { get_resource: fe_8000_pool }
          fe_8080_pool_id: { get_resource: fe_8080_pool }
          fe_443_pool_id: { get_resource: fe_443_pool }
          fe_8443_pool_id: { get_resource: fe_8443_pool }
          metadata: {"metering.stack": {get_param: "OS::stack_id"}}
#          user_data:
#            str_replace:
#              template: |
#                #!/bin/bash -v
#                sudo mysql -uroot -p"password" <<EOF
#                use wordpress;
#                update wp_options set option_value='http://$ip/wordpress' where option_id = 2;
#                update wp_options set option_value='http://$ip/wordpress' where option_id = 1;
#                EOF
#
#              params:
#                $ip:  { get_attr: [lb_floating, floating_ip_address] }
      cooldown: 60
      max_size: 10
      min_size: 3
      desired_capacity: 3
  olap_backends:
    type: OS::Heat::AutoScalingGroup
    properties:
      resource:
        type: olap.yaml
        properties:
          olap_secgroup_tcp_id: { get_resource: olap_secgroup_tcp }
          olap_secgroup_udp_id: { get_resource: olap_secgroup_udp }
          olap_3306_pool_id: { get_resource: olap_3306_pool }
      cooldown: 60
      max_size: 10
      min_size: 4
      desired_capacity: 4
  oltp_backends:
    type: OS::Heat::AutoScalingGroup
    properties:
      resource:
        type: oltp.yaml
        properties:
          oltp_secgroup_tcp_id: { get_resource: oltp_secgroup_tcp }
          oltp_secgroup_udp_id: { get_resource: oltp_secgroup_udp }
          oltp_3306_pool_id: { get_resource: oltp_3306_pool }
      cooldown: 60
      max_size: 10
      min_size: 4
      desired_capacity: 4
  olap_secgroup_tcp:
    type: OS::Neutron::SecurityGroup
    properties:
      name:
        list_join: ['_', [ {get_param: olap_name}, 'secgroup_tcp']]
      rules:
        repeat:
          for_each:
            <%port_min%>: { get_param: olap_sg_port_mins_tcp }
            <%port_max%>: { get_param: olap_sg_port_maxes_tcp }
          template:
            protocol: TCP
            port_range_min: <%port_min%>
            port_range_max: <%port_max%>
          permutations: false
  olap_secgroup_udp:
    type: OS::Neutron::SecurityGroup
    properties:
      name:
        list_join: ['_', [ {get_param: olap_name}, 'secgroup_udp']]
      rules:
        repeat:
          for_each:
            <%port_min%>: { get_param: olap_sg_port_mins_udp }
            <%port_max%>: { get_param: olap_sg_port_maxes_udp }
          template:
            protocol: UDP
            port_range_min: <%port_min%>
            port_range_max: <%port_max%>
          permutations: false
  oltp_secgroup_tcp:
    type: OS::Neutron::SecurityGroup
    properties:
      name:
        list_join: ['_', [ {get_param: oltp_name}, 'secgroup_tcp']]
      rules:
        repeat:
          for_each:
            <%port_min%>: { get_param: oltp_sg_port_mins_tcp }
            <%port_max%>: { get_param: oltp_sg_port_maxes_tcp }
          template:
            protocol: TCP
            port_range_min: <%port_min%>
            port_range_max: <%port_max%>
          permutations: false
  oltp_secgroup_udp:
    type: OS::Neutron::SecurityGroup
    properties:
      name:
        list_join: ['_', [ {get_param: oltp_name}, 'secgroup_udp']]
      rules:
        repeat:
          for_each:
            <%port_min%>: { get_param: oltp_sg_port_mins_udp }
            <%port_max%>: { get_param: oltp_sg_port_maxes_udp }
          template:
            protocol: UDP
            port_range_min: <%port_min%>
            port_range_max: <%port_max%>
          permutations: false
  fe_secgroup_tcp:
    type: OS::Neutron::SecurityGroup
    properties:
      name:
        list_join: ['_', [ {get_param: fe_name}, 'secgroup_tcp']]
      rules:
        repeat:
          for_each:
            <%port_min%>: { get_param: fe_sg_port_mins_tcp }
            <%port_max%>: { get_param: fe_sg_port_maxes_tcp }
          template:
            protocol: TCP
            port_range_min: <%port_min%>
            port_range_max: <%port_max%>
          permutations: false
  fe_secgroup_udp:
    type: OS::Neutron::SecurityGroup
    properties:
      name:
        list_join: ['_', [ {get_param: fe_name}, 'secgroup_udp']]
      rules:
        repeat:
          for_each:
            <%port_min%>: { get_param: fe_sg_port_mins_udp }
            <%port_max%>: { get_param: fe_sg_port_maxes_udp }
          template:
            protocol: TCP
            port_range_min: <%port_min%>
            port_range_max: <%port_max%>
          permutations: false


  fe_80_monitor:
    type: OS::Octavia::HealthMonitor
    properties:
      delay: 3
      type: HTTP
      timeout: 3
      max_retries: 3
      pool: { get_resource: fe_80_pool }
  fe_8000_monitor:
    type: OS::Octavia::HealthMonitor
    properties:
      delay: 3
      type: HTTP
      timeout: 3
      max_retries: 3
      pool: { get_resource: fe_8000_pool }
  fe_8080_monitor:
    type: OS::Octavia::HealthMonitor
    properties:
      delay: 3
      type: HTTP
      timeout: 3
      max_retries: 3
      pool: { get_resource: fe_8080_pool }
  fe_443_monitor:
    type: OS::Octavia::HealthMonitor
    properties:
      delay: 3
      type: HTTPS
      timeout: 3
      max_retries: 3
      pool: { get_resource: fe_443_pool }
  fe_8443_monitor:
    type: OS::Octavia::HealthMonitor
    properties:
      delay: 3
      type: HTTPS
      timeout: 3
      max_retries: 3
      pool: { get_resource: fe_8443_pool }

  fe_80_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: HTTP
      listener: { get_resource: fe_80_listener }
  fe_8000_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: HTTP
      listener: { get_resource: fe_8000_listener }
  fe_8080_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: HTTP
      listener: { get_resource: fe_8080_listener }
  fe_443_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: HTTPS
      listener: { get_resource: fe_443_listener }
  fe_8443_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: HTTPS
      listener: { get_resource: fe_8443_listener }

  fe_80_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: fe_loadbalancer }
      protocol: HTTP
      protocol_port: 80
  fe_8000_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: fe_loadbalancer }
      protocol: HTTP
      protocol_port: 8000
  fe_8080_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: fe_loadbalancer }
      protocol: HTTP
      protocol_port: 8080
  fe_443_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: fe_loadbalancer }
      protocol: HTTPS
      protocol_port: 443
  fe_8443_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: fe_loadbalancer }
      protocol: HTTPS
      protocol_port: 8443

  fe_loadbalancer:
    type: OS::Octavia::LoadBalancer
    properties:
      vip_subnet: { get_param: vip_subnet }
      vip_address: { get_param: fe_vip }

  olap_3306_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: TCP
      listener: { get_resource: olap_3306_listener }
  olap_3306_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: olap_loadbalancer }
      protocol: TCP
      protocol_port: 3306

  olap_loadbalancer:
    type: OS::Octavia::LoadBalancer
    properties:
      vip_subnet: { get_param: vip_subnet }
      vip_address: { get_param: olap_vip }

  oltp_3306_pool:
    type: OS::Octavia::Pool
    properties:
      lb_algorithm: ROUND_ROBIN
      protocol: TCP
      listener: { get_resource: oltp_3306_listener }
  oltp_3306_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: oltp_loadbalancer }
      protocol: TCP
      protocol_port: 3306

  oltp_loadbalancer:
    type: OS::Octavia::LoadBalancer
    properties:
      vip_subnet: { get_param: vip_subnet }
      vip_address: { get_param: oltp_vip }


  fe_scaleup_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: frontends}
      cooldown: 60
      scaling_adjustment: 1
  fe_scaledown_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: frontends}
      cooldown: 60
      scaling_adjustment: -1
  olap_scaleup_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: olap_backends}
      cooldown: 60
      scaling_adjustment: 1
  olap_scaledown_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: olap_backends}
      cooldown: 60
      scaling_adjustment: -1
  oltp_scaleup_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: oltp_backends}
      cooldown: 60
      scaling_adjustment: 1
  oltp_scaledown_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: oltp_backends}
      cooldown: 60
      scaling_adjustment: -1

  fe_cpu_alarm_high:
    type: OS::Ceilometer::Alarm
    properties:
      description: Scale-up if the average CPU > 50% for 1 minute
      meter_name: cpu_util
      statistic: avg
      period: 60
      evaluation_periods: 1
      threshold: 50
      alarm_actions:
        - {get_attr: [fe_scaleup_policy, alarm_url]}
      matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
      comparison_operator: gt
  fe_cpu_alarm_low:
    type: OS::Ceilometer::Alarm
    properties:
      description: Scale-down if the average CPU < 15% for 10 minutes
      meter_name: cpu_util
      statistic: avg
      period: 600
      evaluation_periods: 1
      threshold: 15
      alarm_actions:
        - {get_attr: [fe_scaledown_policy, alarm_url]}
      matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
      comparison_operator: lt
  olap_cpu_alarm_high:
    type: OS::Ceilometer::Alarm
    properties:
      description: Scale-up if the average CPU > 50% for 1 minute
      meter_name: cpu_util
      statistic: avg
      period: 60
      evaluation_periods: 1
      threshold: 50
      alarm_actions:
        - {get_attr: [olap_scaleup_policy, alarm_url]}
      matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
      comparison_operator: gt
  olap_cpu_alarm_low:
    type: OS::Ceilometer::Alarm
    properties:
      description: Scale-down if the average CPU < 15% for 10 minutes
      meter_name: cpu_util
      statistic: avg
      period: 600
      evaluation_periods: 1
      threshold: 15
      alarm_actions:
        - {get_attr: [olap_scaledown_policy, alarm_url]}
      matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
      comparison_operator: lt
  oltp_cpu_alarm_high:
    type: OS::Ceilometer::Alarm
    properties:
      description: Scale-up if the average CPU > 50% for 1 minute
      meter_name: cpu_util
      statistic: avg
      period: 60
      evaluation_periods: 1
      threshold: 50
      alarm_actions:
        - {get_attr: [oltp_scaleup_policy, alarm_url]}
      matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
      comparison_operator: gt
  oltp_cpu_alarm_low:
    type: OS::Ceilometer::Alarm
    properties:
      description: Scale-down if the average CPU < 15% for 10 minutes
      meter_name: cpu_util
      statistic: avg
      period: 600
      evaluation_periods: 1
      threshold: 15
      alarm_actions:
        - {get_attr: [oltp_scaledown_policy, alarm_url]}
      matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
      comparison_operator: lt


outputs:
  fe_vip_ip:
    description: IP Address of the LB VIP
    value: { get_attr: [fe_loadbalancer, vip_address] }

  olap_vip_ip:
    description: IP Address of the OLAP VIP
    value: { get_attr: [olap_loadbalancer, vip_address] }

  oltp_vip_ip:
    description: IP Address of the OLTP VIP
    value: { get_attr: [oltp_loadbalancer, vip_address] }

  fe_scale_up_url:
    description: >
      This URL is the webhook to scale up the autoscaling group.  You
      can invoke the scale-up operation by doing an HTTP POST to this
      URL; no body nor extra headers are needed.
    value: {get_attr: [fe_scaleup_policy, alarm_url]}
  fe_scale_dn_url:
    description: >
      This URL is the webhook to scale down the autoscaling group.
      You can invoke the scale-down operation by doing an HTTP POST to
      this URL; no body nor extra headers are needed.
    value: {get_attr: [fe_scaledown_policy, alarm_url]}
  olap_scale_up_url:
    description: >
      This URL is the webhook to scale up the autoscaling group.  You
      can invoke the scale-up operation by doing an HTTP POST to this
      URL; no body nor extra headers are needed.
    value: {get_attr: [olap_scaleup_policy, alarm_url]}
  olap_scale_dn_url:
    description: >
      This URL is the webhook to scale down the autoscaling group.
      You can invoke the scale-down operation by doing an HTTP POST to
      this URL; no body nor extra headers are needed.
    value: {get_attr: [olap_scaledown_policy, alarm_url]}
  oltp_scale_up_url:
    description: >
      This URL is the webhook to scale up the autoscaling group.  You
      can invoke the scale-up operation by doing an HTTP POST to this
      URL; no body nor extra headers are needed.
    value: {get_attr: [oltp_scaleup_policy, alarm_url]}
  oltp_scale_dn_url:
    description: >
      This URL is the webhook to scale down the autoscaling group.
      You can invoke the scale-down operation by doing an HTTP POST to
      this URL; no body nor extra headers are needed.
    value: {get_attr: [oltp_scaledown_policy, alarm_url]}

  ceilometer_query:
    value:
      str_replace:
        template: >
          ceilometer statistics -m cpu_util
          -q metadata.user_metadata.stack=stackval -p 600 -a avg
        params:
          stackval: { get_param: "OS::stack_id" }
    description: >
      This is a Ceilometer query for statistics on the cpu_util meter
      Samples about OS::Nova::Server instances in this stack.  The -q
      parameter selects Samples according to the subject's metadata.
      When a VM's metadata includes an item of the form metering.X=Y,
      the corresponding Ceilometer resource has a metadata item of the
      form user_metadata.X=Y and samples about resources so tagged can
      be queried with a Ceilometer query term of the form
      metadata.user_metadata.X=Y.  In this case the nested stacks give
      their VMs metadata that is passed as a nested stack parameter,
      and this stack passes a metadata of the form metering.stack=Y,
      where Y is this stack's ID.

